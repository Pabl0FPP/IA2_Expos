# ğŸš€ RAG (Retrieval-Augmented Generation) con MÃ©tricas de EvaluaciÃ³n Avanzadas

Este proyecto implementa un sistema completo de **Retrieval-Augmented Generation (RAG)** utilizando **Google Gemini** y **ChromaDB**, junto con un framework comprensivo de evaluaciÃ³n que incluye mÃºltiples mÃ©tricas automÃ¡ticas para comparar el rendimiento entre respuestas generadas con y sin RAG.

---

## ğŸ‘¥ **Autores**

- **Rafaela SofÃ­a Ruiz Pizarro** - A00395368  
- **Pablo Fernando Pineda PatiÃ±o** - A00395831

---

## ğŸ¯ **Objetivos del Proyecto**

1. **Implementar un sistema RAG funcional** que combine recuperaciÃ³n de informaciÃ³n con generaciÃ³n de texto
2. **Desarrollar mÃ©tricas de evaluaciÃ³n automÃ¡tica** para medir la calidad de respuestas generadas
3. **Comparar sistemÃ¡ticamente** el rendimiento entre RAG y respuestas sin contexto externo
4. **Proporcionar anÃ¡lisis visual** y estadÃ­stico de los resultados obtenidos
5. **Crear una herramienta reutilizable** para futuras evaluaciones de sistemas de IA generativa

---

## ğŸ—ï¸ **Arquitectura del Sistema**

### **Componentes Principales:**

```
ğŸ“„ Documento de Entrada
    â†“
ğŸ”ª Chunking (DivisiÃ³n en fragmentos)
    â†“
ğŸ§  GeneraciÃ³n de Embeddings (SentenceTransformers)
    â†“
ğŸ—„ï¸ Base de Datos Vectorial (ChromaDB)
    â†“
ğŸ” Sistema de RecuperaciÃ³n
    â†“
ğŸ¤– GeneraciÃ³n de Respuestas (Google Gemini)
    â†“
ğŸ“Š EvaluaciÃ³n con MÃºltiples MÃ©tricas
```

### **MÃ©tricas de EvaluaciÃ³n Implementadas:**

- **ğŸ¯ BLEU Score**: PrecisiÃ³n de n-gramas entre candidato y referencia
- **ğŸ“ ROUGE (ROUGE-1, ROUGE-L)**: Recall y F1-score para evaluaciÃ³n de resÃºmenes
- **ğŸ§  BERTScore**: Similitud semÃ¡ntica usando embeddings contextuales
- **ğŸ”„ Similitud SemÃ¡ntica**: Similitud coseno entre embeddings de respuestas

---


## ğŸ› ï¸ **TecnologÃ­as Utilizadas**

### **Backend y RAG:**
- **Python 3.8+**
- **Google Gemini API** - Modelo de lenguaje para generaciÃ³n
- **ChromaDB** - Base de datos vectorial para almacenamiento de embeddings
- **SentenceTransformers** - GeneraciÃ³n de embeddings semÃ¡nticos
- **rank_bm25** - Algoritmo BM25 para recuperaciÃ³n basada en keywords

### **MÃ©tricas y EvaluaciÃ³n:**
- **NLTK** - Natural Language Toolkit para BLEU score
- **rouge-score** - ImplementaciÃ³n de mÃ©tricas ROUGE
- **bert-score** - BERTScore para similitud semÃ¡ntica
- **scikit-learn** - MÃ©tricas adicionales y utilidades

### **VisualizaciÃ³n y AnÃ¡lisis:**
- **Matplotlib** - GrÃ¡ficos y visualizaciones
- **Seaborn** - Visualizaciones estadÃ­sticas avanzadas
- **Pandas** - ManipulaciÃ³n y anÃ¡lisis de datos
- **NumPy** - Operaciones numÃ©ricas

---

## âš™ï¸ **InstalaciÃ³n y ConfiguraciÃ³n**


### **Configurar API Key de Google Gemini**

1. Ve a [Google AI Studio](https://aistudio.google.com/)
2. Crea una API Key
3. Reemplaza en el notebook la lÃ­nea:
   ```python
   os.environ["GEMINI_API_KEY"] = "TU_API_KEY_AQUI"
   ```

---

## ğŸš€ **Uso del Sistema**

### **Ejecutar el Notebook Principal**

1. **Abre Jupyter Notebook:**
   ```bash
   jupyter notebook RAG_con_Metricas_Evaluacion.ipynb
   ```

2. **Ejecuta las celdas en orden:**
   - ğŸ“¦ InstalaciÃ³n de librerÃ­as
   - ğŸ”§ ConfiguraciÃ³n del entorno
   - ğŸ“„ Carga de documentos
   - ğŸ§  CreaciÃ³n del sistema RAG
   - ğŸ“Š EvaluaciÃ³n y comparaciÃ³n
   - ğŸ“ˆ VisualizaciÃ³n de resultados

### **Personalizar el Sistema**

#### **Cambiar el Documento de Entrada:**
```python
# En la secciÃ³n de carga de documentos
document_text = """
Tu texto personalizado aquÃ­...
"""
```

#### **Ajustar ParÃ¡metros de Chunking:**
```python
chunks = chunk_text(document_text, chunk_size=150, overlap=30)
```

#### **Modificar Preguntas de EvaluaciÃ³n:**
```python
test_dataset = [
    {
        "question": "Tu pregunta personalizada",
        "reference": "Respuesta de referencia esperada"
    },
    # AÃ±adir mÃ¡s preguntas...
]
```

---

## ğŸ“Š **Resultados y MÃ©tricas**

### **MÃ©tricas Implementadas:**

| MÃ©trica | DescripciÃ³n | Rango | InterpretaciÃ³n |
|---------|-------------|--------|----------------|
| **BLEU** | PrecisiÃ³n de n-gramas | 0-1 | MÃ¡s alto = mejor coincidencia lÃ©xica |
| **ROUGE-1** | Unigram recall/precision/F1 | 0-1 | MÃ¡s alto = mejor cobertura de contenido |
| **ROUGE-L** | Longest common subsequence | 0-1 | MÃ¡s alto = mejor estructura secuencial |
| **BERTScore** | Similitud semÃ¡ntica contextual | 0-1 | MÃ¡s alto = mejor similitud semÃ¡ntica |
| **Sim. SemÃ¡ntica** | Similitud coseno de embeddings | -1 a 1 | MÃ¡s alto = mayor similitud semÃ¡ntica |

### **Ejemplo de Output:**

```
ğŸ“Š RESUMEN DE RESULTADOS:
============================================================

ğŸ“ˆ Promedios por enfoque:
           bleu  rouge1_f1  rougeL_f1  bertscore_f1  semantic_similarity
approach                                                                  
RAG       0.245      0.389      0.352         0.587                0.712
No-RAG    0.198      0.301      0.267         0.498                0.623

BLEU:
  RAG: 0.2453
  No-RAG: 0.1980
  Diferencia: +0.0473
  Mejora: +23.89%
```

---

## ğŸ”¬ **MetodologÃ­a de EvaluaciÃ³n**

### **Proceso de EvaluaciÃ³n:**

1. **GeneraciÃ³n de Respuestas:**
   - RAG: Usando contexto recuperado
   - No-RAG: Solo conocimiento del modelo

2. **CÃ¡lculo de MÃ©tricas:**
   - ComparaciÃ³n contra respuestas de referencia
   - MÃºltiples mÃ©tricas automÃ¡ticas

3. **AnÃ¡lisis EstadÃ­stico:**
   - Promedios, desviaciones estÃ¡ndar
   - Pruebas de significancia
   - Correlaciones entre mÃ©tricas

4. **VisualizaciÃ³n:**
   - GrÃ¡ficos comparativos
   - AnÃ¡lisis de distribuciones

---

## ğŸ’¡ **Conclusiones y Hallazgos**

### **Principales Observaciones:**

1. **ğŸš€ RAG mejora significativamente** la precisiÃ³n de respuestas especÃ­ficas del dominio
2. **ğŸ“Š BERTScore y Similitud SemÃ¡ntica** son las mÃ©tricas mÃ¡s sensibles a mejoras cualitativas
3. **ğŸ¯ BLEU y ROUGE** capturan bien la precisiÃ³n lÃ©xica pero pueden ser conservadores
4. **âš¡ El overhead computacional** de RAG se justifica por la mejora en calidad

### **Recomendaciones:**

- **Para informaciÃ³n especÃ­fica**: RAG supera consistentemente al modelo base
- **Para conocimiento general**: La diferencia puede ser menor
- **OptimizaciÃ³n recomendada**: Ajustar chunk size segÃºn el dominio
- **EvaluaciÃ³n mÃºltiple**: Usar varias mÃ©tricas para una visiÃ³n completa